<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title> - LLM</title>
    <link href="https://www.newresalhaider.com/tags/llm/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://www.newresalhaider.com"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2023-12-16T00:00:00+00:00</updated>
    <id>https://www.newresalhaider.com/tags/llm/atom.xml</id>
    <entry xml:lang="en">
        <title>AI at Home</title>
        <published>2023-12-16T00:00:00+00:00</published>
        <updated>2023-12-16T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://www.newresalhaider.com/post/aiathome/" type="text/html"/>
        <id>https://www.newresalhaider.com/post/aiathome/</id>
        
        <content type="html">&lt;p&gt;Large Language Models (LLMs), such as those that enable Chat-GPT, have been shown to be incredibly capable for language understanding and generation tasks. With the right prompt, they can answer questions, categorize input, rewrite pieces of text, perform sentiment analysis and more. &lt;&#x2F;p&gt;
&lt;p&gt;As good as these tools are, many of them require data to be sent to a remote server and&#x2F;or take some additional costs to run. This is due to the processing power required to use them and the proprietary nature of the models. However there exist models that can be run locally, even on relatively modest hardware, such as some of the &lt;a href=&quot;https:&#x2F;&#x2F;ai.meta.com&#x2F;llama&#x2F;&quot;&gt;LLaMA&lt;&#x2F;a&gt; models from Meta. Such locally runnable models can enable modern AI setups fully running at home, without the need for data being sent to another party. This article is a brief introduction on how to get one these models up and running.&lt;&#x2F;p&gt;
&lt;figure class=centeredfig&gt;
    &lt;img src=aiathome.jpg&gt;
    
    &lt;figcaption&gt;
        
        &lt;h4&gt;A generated picture of an AI helping at home.&lt;&#x2F;h4&gt;
        
        
    &lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;
&lt;p&gt;The easiest solution that I have found for this purpose is the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui&quot;&gt;text-generation-webui&lt;&#x2F;a&gt; tool. As the name suggests it enables the use of language models by a web based UI and it can do so running only on the local machine. The setup is very straightforward: clone or download the repository, run the start script for your operating system, e.g.: &lt;code&gt;start_wsl.bat&lt;&#x2F;code&gt;for running on WSL, and you are pretty much ready to go.&lt;&#x2F;p&gt;
&lt;p&gt;This tool can use various models, but a nice one to get started is &lt;a href=&quot;https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-7B-GGUF&quot;&gt;Llama-2-7B-GGUF model provided by TheBloke&lt;&#x2F;a&gt;, which is a 7 billion parameter LLaMa2 model. After loading it, we can simply start chatting.&lt;&#x2F;p&gt;
&lt;figure class=centeredfig&gt;
    &lt;img src=aiconversation.png&gt;
    
    &lt;figcaption&gt;
        
        &lt;h4&gt;A short conversation with the AI model.&lt;&#x2F;h4&gt;
        
        
    &lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;
&lt;p&gt;Although this model is on the smaller side it runs very adequately on a 2021 model Asus G14 laptop, and functions well for simple queries and conversations.&lt;&#x2F;p&gt;
&lt;p&gt;There can be a lot of possibilities to explore with such a local setup, especially with larger, more capable models. It also provides an easy way for prototyping, as one can also use an API, that is a local drop-in replacement for the API of OpenAI, to interact with the model. &lt;&#x2F;p&gt;
&lt;p&gt;I hope this article can help you get you started in exploring your AI use cases locally!&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
